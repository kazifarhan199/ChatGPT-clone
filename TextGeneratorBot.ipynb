{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrads/kazi0021/Documents/TextGeneratorBot/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"The LSTM models we will use\"\"\"\n",
    "    def __init__(self, n_vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 100\n",
    "        self.embedding_dim = 100\n",
    "        self.num_layers = 1\n",
    "        self.n_vocab = n_vocab\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size).data.to(device),\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size).data.to(device),\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        sequence_length,\n",
    "        num_line,\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.num_line = num_line\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        self.words_indexes = torch.tensor(self.words_indexes).data.to(device)\n",
    "        \n",
    "    def load_words(self):\n",
    "        with open(self.file_path, encoding='utf-8-sig', errors='ignore') as f:\n",
    "            data = f.read()\n",
    "        if self.num_line:\n",
    "            text = self.text_cleaner(data[:self.num_line])\n",
    "        else:\n",
    "            text = self.text_cleaner(data)\n",
    "        return text.split()\n",
    "    \n",
    "    def text_cleaner(self, text):\n",
    "        text = text.lower()\n",
    "        newString = re.sub(r\"'s\\b\",\"\", text)\n",
    "        # remove punctuations\n",
    "        newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "        long_words=[]\n",
    "        # remove short word\n",
    "        for i in newString.split():\n",
    "            if len(i)>=3:                  \n",
    "                long_words.append(i)\n",
    "        return (\" \".join(long_words)).strip()\n",
    "    \n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.words_indexes[index:index+self.sequence_length],\n",
    "            self.words_indexes[index+1:index+self.sequence_length+1],\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size, max_epochs, sequence_length):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'loss': loss.item(), 'perplexity': torch.exp(loss).item()})\n",
    "            \n",
    "        if loss.item() < 0.2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # lower case text\n",
    "    newString = re.sub(r\"'s\\b\",\"\", text)\n",
    "    # remove punctuations\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    long_words=[]\n",
    "    # remove short word\n",
    "    for i in newString.split():\n",
    "        if len(i)>=3:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "    text = text_cleaner(text)\n",
    "    words = text.split()\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().to('cpu').numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "file_path = \"data/smalltext.train.txt\"\n",
    "dataset = Dataset(file_path, sequence_length, num_line=200_000_000)\n",
    "n_vocab = len(dataset.uniq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing..\n",
      "{'epoch': 0, 'loss': 7.994718551635742, 'perplexity': 2965.255859375}\n",
      "Saving model \n",
      "CPU times: user 8.08 s, sys: 4.55 s, total: 12.6 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Initializing..\")\n",
    "    max_epochs = 10\n",
    "    batch_size = 512\n",
    "    \n",
    "    model = Model(n_vocab).to(device)\n",
    "\n",
    "    train(dataset, model, batch_size, max_epochs, sequence_length)\n",
    "\n",
    "    print(\"Saving model \")\n",
    "    torch.save(model.state_dict(), \"model_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(389189, 100)\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (fc): Linear(in_features=100, out_features=389189, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loded_model = Model(n_vocab).to(device)\n",
    "loded_model.load_state_dict(torch.load(\"model_dict.pkl\"))\n",
    "loded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love apples with the information was the far least the miner here and twice begun allowed open relationship and weapons whom understood compared with the ground and yet year brightened with the property caved and timbers that the gods beheaded and the company bowmen who directed with the exception the alexander louis philippe and sparta they mistaking and introduced here that principality exercised entered the subject the palaces and consequently together but the king the products his religions good humoured features and angrily concealed his ancient felt those the fellow sheriffs are little belonging robert speaks the decision hand the elections with'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(predict(dataset, loded_model, \"I love apples\", next_words=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acee745ea36124c453649093d15c25ce11ab58ace9f4418b323ee90ea4940c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
